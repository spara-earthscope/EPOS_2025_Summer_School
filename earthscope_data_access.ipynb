{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1807a59f-fb9f-4831-b0c1-5f10332e2135",
   "metadata": {},
   "source": [
    "# How to Access EarthScope Data\n",
    "\n",
    "EarthScope maintains archives of seismic and geodetic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40d054-4e6c-4209-923a-db7e050050e8",
   "metadata": {},
   "source": [
    "## Getting Seismic Data\n",
    "\n",
    "Commandline download tools:\n",
    "- Rover  –  https://earthscope.github.io/rover/\n",
    "- FetchData  –  https://earthscope.github.io/fetch-scripts/\n",
    "\n",
    "Webservices:\n",
    "- Dataselect - https://service.iris.edu/\n",
    "\n",
    "Python packages:\n",
    "- Obspy\n",
    "- MsPASS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c621d0e-69da-4589-814e-cb4301815b8d",
   "metadata": {},
   "source": [
    "### Rover\n",
    "\n",
    "ROVER is a command line tool to robustly retrieve geophysical timeseries data from data centers such as EarthScope. It builds an associated index for downloaded data to generate a local repository. ROVER compares a built local index to timeseries availability information provided by the datacenter. This enables a local archive to remain synchronized with a remote data center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c93555e9-97c4-4d91-97e4-af7e0aec22a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mseedindex==3.0.4 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (3.0.4)\n",
      "Requirement already satisfied: rover in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: mseedindex>=3.0.4 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from rover) (3.0.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from rover) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from requests>=2.26.0->rover) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from requests>=2.26.0->rover) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from requests>=2.26.0->rover) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from requests>=2.26.0->rover) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir mseedindex==3.0.4\n",
    "!pip install --no-cache-dir rover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401eb218-6cf0-4251-ba76-a1ec12d9f9e8",
   "metadata": {},
   "source": [
    "Run rover in a terminal\n",
    "\n",
    "```\n",
    "rover init-repository datarepo\n",
    "cd datarepo\n",
    "```\n",
    "\n",
    "Run the process rover retrieve to fetch these data:\n",
    "\n",
    "```\n",
    "rover retrieve request.txt\n",
    "```\n",
    "\n",
    "`list-summary` prints the retrieved data from the earliest to the latest timespans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd93c79-ede1-4d44-a244-eaf3c2b2c894",
   "metadata": {},
   "source": [
    "### FetchData and FetchEvent\n",
    "\n",
    "These scripts are require perl to run.\n",
    "\n",
    "**FetchData** - Fetch time series and optionally, related metadata, matching SAC Poles and Zeros and matching SEED RESP files. Time series data are returned in miniSEED format, and metadata is saved as a simple ASCII list.\n",
    "\n",
    "To download the script in GeoLab and make it executable:\n",
    "\n",
    "```\n",
    "url -O https://earthscope.github.io/fetch-scripts/FetchData\n",
    "chmod +x FetchData\n",
    "```\n",
    "\n",
    "Usage documentation:\n",
    "\n",
    "```\n",
    "./FetchData\n",
    "```\n",
    "\n",
    "Example usage:\n",
    "\n",
    "To request the first hour of the year 2011 for BHZ channels from GSN stations, execute the following command:\n",
    "\n",
    "```\n",
    "./FetchData -N _GSN -C BHZ -s 2011-01-01T00:00:00 -e 2011-01-01T01:00:00 -o GSN.mseed -m GSN.metadata\n",
    "```\n",
    "\n",
    "**FetchEvent** - Fetch event parameters and print simple text summary. Works with any fdsnws-event service.\n",
    "\n",
    "To download the script in GeoLab and make it executable:\n",
    "\n",
    "```\n",
    "url -O https://earthscope.github.io/fetch-scripts/FetchEvent\n",
    "chmod +x FetchData\n",
    "```\n",
    "\n",
    "Usage documentation:\n",
    "\n",
    "```\n",
    "./FetchEvent\n",
    "```\n",
    "\n",
    "Example usage:\n",
    "\n",
    "To request magnitude 6+ events within 20 degrees of the main shock of the Tohoku-Oki, Japan Earthquake on or after March 11th 2011, execute the following command:\n",
    "\n",
    "```\n",
    "./FetchEvent -s 2011-03-11 --radius 38.2:142.3:20 --mag 6\n",
    "```\n",
    "\n",
    "More information about FetchData and FetchEvent is available on this [page](https://earthscope.github.io/fetch-scripts/docs/tutorial/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a8def-3a50-4190-9a04-a3b04aeb1778",
   "metadata": {},
   "source": [
    "### Dataselect to GeoLab directory\n",
    "\n",
    "Dataselect is a web service for downloading data from the SAGE archive. API documentation is available on this [page](https://service.earthscope.org/fdsnws/dataselect/1/).\n",
    "\n",
    "The following example downloads miniseed files to GeoLab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af6b76fb-cdb7-43a7-9dec-1092f7c30212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv, os\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "# SAGE archive\n",
    "URL = \"http://service.iris.edu/fdsnws/dataselect/1/query?\"\n",
    "\n",
    "def download(station, directory_path):\n",
    "    # calculate duration\n",
    "    start_year = int(station[\"starttime\"][:4])\n",
    "    end_year = int(station[\"endtime\"][:4])\n",
    "    startdate = station[\"starttime\"].split(\"T\")[0]\n",
    "    enddate = station[\"endtime\"].split(\"T\")[0]\n",
    "    days = date.fromisoformat(enddate) - date.fromisoformat(startdate)\n",
    "    total_days = days.days\n",
    "    \n",
    "    # duration\n",
    "    start_year = int(station[\"starttime\"][:4])\n",
    "    end_year = int(station[\"endtime\"][:4])\n",
    "    params = {\"net\" : station[\"network\"],\n",
    "              \"sta\" : station[\"station\"],\n",
    "              \"loc\" : station[\"location\"],\n",
    "              \"cha\" : station[\"channel\"],\n",
    "              \"start\": station[\"starttime\"],\n",
    "              \"end\": station[\"endtime\"]}\n",
    "    \n",
    "    # download miniseed to local drive\n",
    "    for day in range(1,total_days + 1):\n",
    "        # this only works for 2 years\n",
    "        if day > 366:\n",
    "            year = end_year\n",
    "        else:\n",
    "            year = start_year\n",
    "\n",
    "        # file name format: STATION.NETWORK.YEAR.DAYOFYEAR\n",
    "        file_name = \".\".join([station[\"station\"], station[\"network\"], str(year), \"{:03d}\".format(day)])\n",
    "\n",
    "        r = requests.get(URL, params=params, stream=True)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # save the file\n",
    "            with open(Path(Path(directory_path) / file_name), 'wb') as f:\n",
    "                for data in r:\n",
    "                    f.write(data)\n",
    "        else:\n",
    "            #problem occured\n",
    "            print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "\n",
    "# create directory for data\n",
    "directory_path = \"./miniseed_data\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "stations_file = \"five_stations.csv\"\n",
    "\n",
    "with open(stations_file, 'r') as file:\n",
    "    csv_reader = csv.DictReader(file, delimiter=',', doublequote=False)\n",
    "    for row in csv_reader:\n",
    "        download(row, directory_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2f352-b6cd-4596-8fa9-375816b40b74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Using Dataselect to copy miniseed to AWS S3\n",
    "\n",
    "EarthScope will make the SAGE archive available on AWS S3 later this year. This means you can access miniseed data in the cloud and process it without downloading the file to GeoLab. The data is adjacent to GeoLab, which reduces the time to access the file and process it. For the purpose of demonstrating working with data in the cloud, this example demonstrates how to copy miniseed files from EarthScope to an AWS S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0e3a9b-46a0-4246-b0c3-d1a6a576addb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied WCI.IU.2014.001 to WCI/2014/001/WCI.IU.2014.001 in S3\n",
      "copied KBS.IU.2014.001 to KBS/2014/001/KBS.IU.2014.001 in S3\n",
      "copied TIXI.IU.2014.001 to TIXI/2014/001/TIXI.IU.2014.001 in S3\n",
      "copied KIV.II.2014.001 to KIV/2014/001/KIV.II.2014.001 in S3\n",
      "copied TRIS.G.2014.001 to TRIS/2014/001/TRIS.G.2014.001 in S3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv, os\n",
    "import pathlib as Path\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import date\n",
    "\n",
    "# s3 setup, change profile name\n",
    "session = boto3.Session(profile_name=\"spara\")\n",
    "s3_client = session.client('s3')\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = \"my-miniseed\"\n",
    "region_name = \"us-east-2\"\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket = bucket_name)\n",
    "except ClientError as error:\n",
    "    error_code = int(error.response['Error']['Code'])\n",
    "    if  error_code == 404:\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region_name})\n",
    "\n",
    "\n",
    "# SAGE archive\n",
    "URL = \"http://service.iris.edu/fdsnws/dataselect/1/query?\"\n",
    "\n",
    "def upload_to_s3(station):\n",
    "    # calculate duration\n",
    "    start_year = int(station[\"starttime\"][:4])\n",
    "    end_year = int(station[\"endtime\"][:4])\n",
    "    startdate = station[\"starttime\"].split(\"T\")[0]\n",
    "    enddate = station[\"endtime\"].split(\"T\")[0]\n",
    "    days = date.fromisoformat(enddate) - date.fromisoformat(startdate)\n",
    "    total_days = days.days\n",
    "\n",
    "    # set params for request\n",
    "    params = {\"net\" : station[\"network\"],\n",
    "              \"sta\" : station[\"station\"],\n",
    "              \"loc\" : station[\"location\"],\n",
    "              \"cha\" : station[\"channel\"],\n",
    "              \"start\": station[\"starttime\"],\n",
    "              \"end\": station[\"endtime\"]}\n",
    "    \n",
    "    # upload miniseed to s3\n",
    "    for day in range(1,total_days + 1):\n",
    "        if day > 366:\n",
    "            year = end_year\n",
    "        else:\n",
    "            year = start_year\n",
    "\n",
    "        # file name format: STATION.NETWORK.YEAR.DAYOFYEAR\n",
    "        # bucket path format: 'miniseed/TA/2004/365/A04A.TA.2004.365#2'\n",
    "        doy = f\"{str(day):0>3}\"\n",
    "        s3_path_prefix = \"/\".join([station[\"station\"], str(year), doy])\n",
    "        file = \".\".join([station[\"station\"], station[\"network\"], str(year), doy])\n",
    "        key = \"/\".join([s3_path_prefix,file])\n",
    "\n",
    "        # copy data from archive to S3\n",
    "        r = requests.get(URL, params=params, stream=True)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # save the file\n",
    "            bucket = s3.Bucket(bucket_name)\n",
    "            bucket.upload_fileobj(r.raw, key)\n",
    "            print(\"copied %s to %s in S3\" % (file, key))\n",
    "        else:\n",
    "            #problem occured\n",
    "            print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "# parse stations\n",
    "stations_file = \"five_stations.csv\"\n",
    "\n",
    "with open(stations_file, 'r') as file:\n",
    "    csv_reader = csv.DictReader(file, delimiter=',', doublequote=False)\n",
    "    for row in csv_reader:\n",
    "        upload_to_s3(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdaed5-75af-4fa2-8946-43c6afb632b0",
   "metadata": {},
   "source": [
    "### Using a miniseed file from S3\n",
    "\n",
    "This example demonstrates how to read a single miniseed file and read the data with obspy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f93cb866-415d-480b-b964-e3ef035219b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Trace(s) in Stream:\n",
      "IU.WCI.10.BH1 | 2014-01-01T00:00:00.019500Z - 2014-01-01T23:59:59.994500Z | 40.0 Hz, 3456000 samples\n",
      "IU.WCI.10.BH2 | 2014-01-01T00:00:00.019500Z - 2014-01-01T23:59:59.994500Z | 40.0 Hz, 3456000 samples\n",
      "IU.WCI.10.BHZ | 2014-01-01T00:00:00.019500Z - 2014-01-01T23:59:59.994500Z | 40.0 Hz, 3456000 samples\n",
      "IU.WCI.10.LH1 | 2014-01-01T00:00:00.069500Z - 2014-01-01T23:59:59.069500Z | 1.0 Hz, 86400 samples\n",
      "IU.WCI.10.LH2 | 2014-01-01T00:00:00.069500Z - 2014-01-01T23:59:59.069500Z | 1.0 Hz, 86400 samples\n",
      "IU.WCI.10.LHZ | 2014-01-01T00:00:00.069500Z - 2014-01-01T23:59:59.069500Z | 1.0 Hz, 86400 samples\n",
      "IU.WCI.10.VH1 | 2014-01-01T00:00:00.069500Z - 2014-01-01T23:59:50.069500Z | 0.1 Hz, 8640 samples\n",
      "IU.WCI.10.VH2 | 2014-01-01T00:00:00.069500Z - 2014-01-01T23:59:50.069500Z | 0.1 Hz, 8640 samples\n",
      "IU.WCI.10.VHZ | 2014-01-01T00:00:00.069500Z - 2014-01-01T23:59:50.069500Z | 0.1 Hz, 8640 samples\n",
      "IU.WCI.10.VMU | 2014-01-01T00:00:09.000000Z - 2014-01-01T23:59:59.000000Z | 0.1 Hz, 8640 samples\n",
      "IU.WCI.10.VMV | 2014-01-01T00:00:09.000000Z - 2014-01-01T23:59:59.000000Z | 0.1 Hz, 8640 samples\n",
      "IU.WCI.10.VMW | 2014-01-01T00:00:09.000000Z - 2014-01-01T23:59:59.000000Z | 0.1 Hz, 8640 samples\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import date\n",
    "from obspy import read\n",
    "import io \n",
    "\n",
    "# s3 setup\n",
    "session = boto3.Session(profile_name=\"spara\")\n",
    "s3_client = session.client('s3')\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = \"my-miniseed\"\n",
    "region_name = \"us-east-2\"\n",
    "\n",
    "# Define bucket and key\n",
    "bucket_name = 'my-miniseed'\n",
    "object_key = 'WCI/2014/001/WCI.IU.2014.001'\n",
    "\n",
    "# Download object to memory\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "data_stream = io.BytesIO(response['Body'].read())\n",
    "\n",
    "# Parse with ObsPy\n",
    "st = read(data_stream)\n",
    "\n",
    "# Print the ObsPy Streams\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a9228-8859-40db-9397-98122d0d3a45",
   "metadata": {},
   "source": [
    "## Geodetic Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07919514-2b71-411a-b561-60e3584ac4f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting earthscope_sdk==1.0.0b1\n",
      "  Using cached earthscope_sdk-1.0.0b1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from earthscope_sdk==1.0.0b1) (0.28.1)\n",
      "Collecting pydantic-settings>=2.8.0 (from pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting stamina>=24.3.0 (from earthscope_sdk==1.0.0b1)\n",
      "  Using cached stamina-25.1.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: anyio in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from httpx>=0.27.0->earthscope_sdk==1.0.0b1) (4.7.0)\n",
      "Requirement already satisfied: certifi in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from httpx>=0.27.0->earthscope_sdk==1.0.0b1) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from httpx>=0.27.0->earthscope_sdk==1.0.0b1) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from httpx>=0.27.0->earthscope_sdk==1.0.0b1) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->earthscope_sdk==1.0.0b1) (0.16.0)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic-settings>=2.8.0->pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (2.10.3)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic-settings>=2.8.0->pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic-settings>=2.8.0->pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic>=2.7.0->pydantic-settings>=2.8.0->pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic>=2.7.0->pydantic-settings>=2.8.0->pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic>=2.7.0->pydantic-settings>=2.8.0->pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (4.12.2)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from pydantic-settings[toml]>=2.8.0->earthscope_sdk==1.0.0b1) (2.0.1)\n",
      "Requirement already satisfied: tenacity in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from stamina>=24.3.0->earthscope_sdk==1.0.0b1) (9.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sophiaparafina/miniconda3/envs/epos/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->earthscope_sdk==1.0.0b1) (1.3.0)\n",
      "Using cached earthscope_sdk-1.0.0b1-py3-none-any.whl (34 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached stamina-25.1.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: stamina, pydantic-settings, earthscope_sdk\n",
      "\u001b[2K  Attempting uninstall: pydantic-settings\n",
      "\u001b[2K    Found existing installation: pydantic-settings 2.6.1\n",
      "\u001b[2K    Uninstalling pydantic-settings-2.6.1:\n",
      "\u001b[2K      Successfully uninstalled pydantic-settings-2.6.1\n",
      "\u001b[2K  Attempting uninstall: earthscope_sdk\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [pydantic-settings]\n",
      "\u001b[2K    Found existing installation: earthscope-sdk 0.2.18;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [pydantic-settings]\n",
      "\u001b[2K    Uninstalling earthscope-sdk-0.2.1:;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [pydantic-settings]\n",
      "\u001b[2K      Successfully uninstalled earthscope-sdk-0.2.1\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [earthscope_sdk]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [earthscope_sdk]━━━\u001b[0m \u001b[32m2/3\u001b[0m [earthscope_sdk]\n",
      "Successfully installed earthscope_sdk-1.0.0b1 pydantic-settings-2.9.1 stamina-25.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install earthscope_sdk==1.0.0b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63aabada-8978-47f0-875e-e1271e7a02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/001/p0380010.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/002/p0380020.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/003/p0380030.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/004/p0380040.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/005/p0380050.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/006/p0380060.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/007/p0380070.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/008/p0380080.24d.Z\n",
      "downloading:  https://gage-data.earthscope.org/archive/gnss/rinex/obs/2024/009/p0380090.24d.Z\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os, json\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    " \n",
    "from earthscope_sdk import EarthScopeClient\n",
    "\n",
    "client = EarthScopeClient()\n",
    "\n",
    "def get_token(token_path='./'):\n",
    "\n",
    "    # refresh the token if it has expired\n",
    "    client.ctx.auth_flow.refresh_if_necessary()\n",
    "\n",
    "    token = client.ctx.auth_flow.access_token\n",
    "    \n",
    "    return token\n",
    "\n",
    "def get_es_file(url, directory_to_save_file='./', token_path='./'):\n",
    "\n",
    "  # get authorization Bearer token\n",
    "  token = get_token()\n",
    "\n",
    "  # request a file and provide the token in the Authorization header\n",
    "  file_name = Path(url).name\n",
    "\n",
    "  r = requests.get(url, headers={\"authorization\": f\"Bearer {token}\"})\n",
    "  if r.status_code == requests.codes.ok:\n",
    "    # save the file\n",
    "    with open(Path(Path(directory_to_save_file) / file_name), 'wb') as f:\n",
    "        for data in r:\n",
    "            f.write(data)\n",
    "  else:\n",
    "    #problem occured\n",
    "    print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "    # https://gage-data.earthscope.org/archive/gnss/rinex/obs/<year>/<day>/<station><day>0.<two digit year>d.Z\n",
    "\n",
    "directory_path = \"./rinex_data\"\n",
    "\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "def download_rinex(doy, year, station):\n",
    "    two_digit_year=str(year)[2:] #converts integer to string and slices the last characters\n",
    "    for doy in np.arange(1,10):\n",
    "        #download\n",
    "        url='https://gage-data.earthscope.org/archive/gnss/rinex/obs/%d/%03d/%s%03d0.%sd.Z' %(year,doy,station,doy,two_digit_year)\n",
    "        print('downloading: ', url)\n",
    "        get_es_file(url, 'rinex_data')\n",
    "\n",
    "\n",
    "station = \"p038\"\n",
    "doy = 1\n",
    "year = 2024\n",
    "\n",
    "download_rinex(doy, year, station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d69ab5-a8df-46aa-957a-bda1bdc3e079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
