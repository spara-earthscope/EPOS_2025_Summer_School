{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1807a59f-fb9f-4831-b0c1-5f10332e2135",
   "metadata": {},
   "source": [
    "# How to Access EarthScope Data\n",
    "\n",
    "EarthScope maintains archives of seismic and geodetic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40d054-4e6c-4209-923a-db7e050050e8",
   "metadata": {},
   "source": [
    "## Getting Seismic Data\n",
    "\n",
    "Commandline download tools:\n",
    "- Rover  –  https://earthscope.github.io/rover/\n",
    "- FetchData  –  https://earthscope.github.io/fetch-scripts/\n",
    "\n",
    "Webservices:\n",
    "- Dataselect - https://service.iris.edu/\n",
    "\n",
    "Python packages:\n",
    "- [obspy](https://docs.obspy.org/)\n",
    "- [msPASS](https://www.mspass.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c621d0e-69da-4589-814e-cb4301815b8d",
   "metadata": {},
   "source": [
    "### Rover\n",
    "\n",
    "ROVER is a command line tool to robustly retrieve geophysical timeseries data from data centers such as EarthScope. It builds an associated index for downloaded data to generate a local repository. ROVER compares a built local index to timeseries availability information provided by the datacenter. This enables a local archive to remain synchronized with a remote data center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93555e9-97c4-4d91-97e4-af7e0aec22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir rover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401eb218-6cf0-4251-ba76-a1ec12d9f9e8",
   "metadata": {},
   "source": [
    "Run rover in a terminal\n",
    "\n",
    "```\n",
    "rover init-repository datarepo\n",
    "cd datarepo\n",
    "```\n",
    "\n",
    "Run the process rover retrieve to fetch these data:\n",
    "\n",
    "```\n",
    "rover retrieve request.txt\n",
    "```\n",
    "\n",
    "`list-summary` prints the retrieved data from the earliest to the latest timespans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd93c79-ede1-4d44-a244-eaf3c2b2c894",
   "metadata": {},
   "source": [
    "### FetchData and FetchEvent\n",
    "\n",
    "These scripts require [perl](https://www.perl.org/about.html) to run. Perl is included in GeoLab but you may need to [install](https://www.perl.org/get.html) it on your computer if it is not present.\n",
    "\n",
    "**FetchData** - Fetch time series and optionally, related metadata, matching SAC Poles and Zeros and matching SEED RESP files. Time series data are returned in miniSEED format, and metadata is saved as a simple ASCII list.\n",
    "\n",
    "The FetchData script is included in this repository, but you can download the script to your computer and make it executable:\n",
    "\n",
    "```\n",
    "curl -O https://earthscope.github.io/fetch-scripts/FetchData\n",
    "chmod +x FetchData\n",
    "```\n",
    "\n",
    "Usage documentation:\n",
    "\n",
    "```\n",
    "./FetchData\n",
    "```\n",
    "\n",
    "Example usage:\n",
    "\n",
    "To request the first hour of the year 2011 for BHZ channels from GSN stations, execute the following command:\n",
    "\n",
    "```\n",
    "./FetchData -N _GSN -C BHZ -s 2011-01-01T00:00:00 -e 2011-01-01T01:00:00 -o GSN.mseed -m GSN.metadata\n",
    "```\n",
    "\n",
    "**FetchEvent** - Fetch event parameters and print simple text summary. Works with any fdsnws-event service.\n",
    "\n",
    "The FetchEvent script is included in this repository, but you can download the script to your computer and make it executable:\n",
    "\n",
    "```\n",
    "curl -O https://earthscope.github.io/fetch-scripts/FetchEvent\n",
    "chmod +x FetchData\n",
    "```\n",
    "\n",
    "Usage documentation:\n",
    "\n",
    "```\n",
    "./FetchEvent\n",
    "```\n",
    "\n",
    "Example usage:\n",
    "\n",
    "To request magnitude 6+ events within 20 degrees of the main shock of the Tohoku-Oki, Japan Earthquake on or after March 11th 2011, execute the following command:\n",
    "\n",
    "```\n",
    "./FetchEvent -s 2011-03-11 --radius 38.2:142.3:20 --mag 6\n",
    "```\n",
    "\n",
    "More information about FetchData and FetchEvent is available on this [page](https://earthscope.github.io/fetch-scripts/docs/tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a8def-3a50-4190-9a04-a3b04aeb1778",
   "metadata": {},
   "source": [
    "### Dataselect to GeoLab directory\n",
    "\n",
    "Dataselect is a web service for downloading data from the SAGE archive. API documentation is available on this [page](https://service.earthscope.org/fdsnws/dataselect/1/).\n",
    "\n",
    "The following example downloads miniseed files to GeoLab from the `five_stations.csv` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b76fb-cdb7-43a7-9dec-1092f7c30212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv, os\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "# SAGE archive\n",
    "URL = \"http://service.iris.edu/fdsnws/dataselect/1/query?\"\n",
    "\n",
    "def download(station, directory_path):\n",
    "    # calculate duration\n",
    "    start_year = int(station[\"starttime\"][:4])\n",
    "    end_year = int(station[\"endtime\"][:4])\n",
    "    startdate = station[\"starttime\"].split(\"T\")[0]\n",
    "    enddate = station[\"endtime\"].split(\"T\")[0]\n",
    "    days = date.fromisoformat(enddate) - date.fromisoformat(startdate)\n",
    "    total_days = days.days\n",
    "    \n",
    "    # duration\n",
    "    start_year = int(station[\"starttime\"][:4])\n",
    "    end_year = int(station[\"endtime\"][:4])\n",
    "    params = {\"net\" : station[\"network\"],\n",
    "              \"sta\" : station[\"station\"],\n",
    "              \"loc\" : station[\"location\"],\n",
    "              \"cha\" : station[\"channel\"],\n",
    "              \"start\": station[\"starttime\"],\n",
    "              \"end\": station[\"endtime\"]}\n",
    "    \n",
    "    # download miniseed to local drive\n",
    "    for day in range(1,total_days + 1):\n",
    "        # this only works for 2 years\n",
    "        if day > 366:\n",
    "            year = end_year\n",
    "        else:\n",
    "            year = start_year\n",
    "\n",
    "        # file name format: STATION.NETWORK.YEAR.DAYOFYEAR\n",
    "        file_name = \".\".join([station[\"station\"], station[\"network\"], str(year), \"{:03d}\".format(day)])\n",
    "\n",
    "        r = requests.get(URL, params=params, stream=True)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # save the file\n",
    "            with open(Path(Path(directory_path) / file_name), 'wb') as f:\n",
    "                for data in r:\n",
    "                    f.write(data)\n",
    "        else:\n",
    "            #problem occured\n",
    "            print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "\n",
    "# create directory for data\n",
    "directory_path = \"./miniseed_data\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "stations_file = \"five_stations.csv\"\n",
    "\n",
    "with open(stations_file, 'r') as file:\n",
    "    csv_reader = csv.DictReader(file, delimiter=',', doublequote=False)\n",
    "    for row in csv_reader:\n",
    "        download(row, directory_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2f352-b6cd-4596-8fa9-375816b40b74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Using Dataselect to copy miniseed to AWS S3\n",
    "\n",
    "EarthScope will make the SAGE archive available on AWS S3 later this year. This means you can access miniseed data in the cloud and process it without downloading the file to GeoLab. The data is adjacent to GeoLab, which reduces the time to access the file and process it. For the purpose of demonstrating working with data in the cloud, this example shows     how to copy miniseed files from EarthScope to an AWS S3 bucket.\n",
    "\n",
    "> You will need to an AWS account to use these scripts. Make sure you have an [AWS credentials](https://docs.aws.amazon.com/cli/v1/userguide/cli-chap-authentication.html) file in the `./aws` directory, which is in your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e3a9b-46a0-4246-b0c3-d1a6a576addb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv, os\n",
    "import pathlib as Path\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import date\n",
    "\n",
    "# s3 setup, change profile name\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3_client = session.client('s3')\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = \"my-miniseed\"\n",
    "region_name = \"us-east-2\"\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket = bucket_name)\n",
    "except ClientError as error:\n",
    "    error_code = int(error.response['Error']['Code'])\n",
    "    if  error_code == 404:\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region_name})\n",
    "\n",
    "\n",
    "# SAGE archive\n",
    "URL = \"http://service.iris.edu/fdsnws/dataselect/1/query?\"\n",
    "\n",
    "def upload_to_s3(station):\n",
    "    # calculate duration\n",
    "    start_year = int(station[\"starttime\"][:4])\n",
    "    end_year = int(station[\"endtime\"][:4])\n",
    "    startdate = station[\"starttime\"].split(\"T\")[0]\n",
    "    enddate = station[\"endtime\"].split(\"T\")[0]\n",
    "    days = date.fromisoformat(enddate) - date.fromisoformat(startdate)\n",
    "    total_days = days.days\n",
    "\n",
    "    # set params for request\n",
    "    params = {\"net\" : station[\"network\"],\n",
    "              \"sta\" : station[\"station\"],\n",
    "              \"loc\" : station[\"location\"],\n",
    "              \"cha\" : station[\"channel\"],\n",
    "              \"start\": station[\"starttime\"],\n",
    "              \"end\": station[\"endtime\"]}\n",
    "    \n",
    "    # upload miniseed to s3\n",
    "    for day in range(1,total_days + 1):\n",
    "        if day > 366:\n",
    "            year = end_year\n",
    "        else:\n",
    "            year = start_year\n",
    "\n",
    "        # file name format: STATION.NETWORK.YEAR.DAYOFYEAR\n",
    "        # bucket path format: 'miniseed/TA/2004/365/A04A.TA.2004.365#2'\n",
    "        doy = f\"{str(day):0>3}\"\n",
    "        s3_path_prefix = \"/\".join([station[\"station\"], str(year), doy])\n",
    "        file = \".\".join([station[\"station\"], station[\"network\"], str(year), doy])\n",
    "        key = \"/\".join([s3_path_prefix,file])\n",
    "\n",
    "        # copy data from archive to S3\n",
    "        r = requests.get(URL, params=params, stream=True)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # save the file\n",
    "            bucket = s3.Bucket(bucket_name)\n",
    "            bucket.upload_fileobj(r.raw, key)\n",
    "            print(\"copied %s to %s in S3\" % (file, key))\n",
    "        else:\n",
    "            #problem occured\n",
    "            print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "# parse stations\n",
    "stations_file = \"five_stations.csv\"\n",
    "\n",
    "with open(stations_file, 'r') as file:\n",
    "    csv_reader = csv.DictReader(file, delimiter=',', doublequote=False)\n",
    "    for row in csv_reader:\n",
    "        upload_to_s3(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdaed5-75af-4fa2-8946-43c6afb632b0",
   "metadata": {},
   "source": [
    "### Using a miniseed file from S3\n",
    "\n",
    "This example demonstrates how to read a single miniseed file and read the data with obspy. The advantage of this method is the data is streamed directly to obspy and skips the step of downloading the data. This more efficient when processing a large volume of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cb866-415d-480b-b964-e3ef035219b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import date\n",
    "from obspy import read\n",
    "import io \n",
    "\n",
    "# s3 setup\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3_client = session.client('s3')\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = \"my-miniseed\"\n",
    "region_name = \"us-east-2\"\n",
    "\n",
    "# Define bucket and key\n",
    "bucket_name = 'my-miniseed'\n",
    "object_key = 'WCI/2014/001/WCI.IU.2014.001'\n",
    "\n",
    "# Download object to memory\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "data_stream = io.BytesIO(response['Body'].read())\n",
    "\n",
    "# Parse with ObsPy\n",
    "st = read(data_stream)\n",
    "\n",
    "# Print the ObsPy Streams\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a9228-8859-40db-9397-98122d0d3a45",
   "metadata": {},
   "source": [
    "## Geodetic Data\n",
    "\n",
    "Geodetic from the [GAGE archive](https://gage-data.earthscope.org/archive) requires authentication (see the `get_token` function). This example downloads a 10 days of GNSS data from GNSS station p038."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aabada-8978-47f0-875e-e1271e7a02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os, json\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    " \n",
    "from earthscope_sdk import EarthScopeClient\n",
    "\n",
    "client = EarthScopeClient()\n",
    "\n",
    "def get_token(token_path='./'):\n",
    "\n",
    "    # refresh the token if it has expired\n",
    "    client.ctx.auth_flow.refresh_if_necessary()\n",
    "\n",
    "    token = client.ctx.auth_flow.access_token\n",
    "    \n",
    "    return token\n",
    "\n",
    "def get_es_file(url, directory_to_save_file='./', token_path='./'):\n",
    "\n",
    "  # get authorization Bearer token\n",
    "  token = get_token()\n",
    "\n",
    "  # request a file and provide the token in the Authorization header\n",
    "  file_name = Path(url).name\n",
    "\n",
    "  r = requests.get(url, headers={\"authorization\": f\"Bearer {token}\"})\n",
    "  if r.status_code == requests.codes.ok:\n",
    "    # save the file\n",
    "    with open(Path(Path(directory_to_save_file) / file_name), 'wb') as f:\n",
    "        for data in r:\n",
    "            f.write(data)\n",
    "  else:\n",
    "    #problem occured\n",
    "    print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "\n",
    "    # https://gage-data.earthscope.org/archive/gnss/rinex/obs/<year>/<day>/<station><day>0.<two digit year>d.Z\n",
    "\n",
    "directory_path = \"./rinex_data\"\n",
    "\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "def download_rinex(doy, year, station):\n",
    "    two_digit_year=str(year)[2:] #converts integer to string and slices the last characters\n",
    "    for doy in np.arange(1,10):\n",
    "        #download\n",
    "        url='https://gage-data.earthscope.org/archive/gnss/rinex/obs/%d/%03d/%s%03d0.%sd.Z' %(year,doy,station,doy,two_digit_year)\n",
    "        print('downloading: ', url)\n",
    "        get_es_file(url, 'rinex_data')\n",
    "\n",
    "\n",
    "station = \"p038\"\n",
    "doy = 1\n",
    "year = 2024\n",
    "\n",
    "download_rinex(doy, year, station)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
